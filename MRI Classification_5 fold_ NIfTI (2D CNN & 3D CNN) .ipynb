{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conference paper_2D CNN outperforms 3D CNN in Small Dataset Classification\n",
    "\n",
    "# 2D and 3D CNN models were trained on a personal computer (NVIDIA TITAN RTX GPU, Python 3.7.9, TensorFlow 2.1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import join, basename, isdir\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.color import gray2rgb\n",
    "from tqdm.notebook import trange, tqdm, tqdm_notebook\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import h5py\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as im\n",
    "import matplotlib.image as img\n",
    "import nibabel as nb\n",
    "from nibabel.testing import data_path\n",
    "import SimpleITK as sitk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeGImkM1TYr3",
    "outputId": "da3dc1b5-dd01-4b1b-8a89-359ea841d224"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "# If you use a recent tensorflow, you HAVE TO use the tensorflow.keras exclusively, or you can get weird results from version mismatches.\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model, save_model, Sequential   #to creat hirachical layers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout,SpatialDropout2D, Flatten, Conv3D, Conv2D, MaxPool3D, MaxPool2D, GlobalAveragePooling3D, GlobalAveragePooling2D, Flatten, Input, BatchNormalization, GRU, Bidirectional,Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback\n",
    "from tensorflow.keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib.pyplot import cm\n",
    "# more info on callbakcs: https://keras.io/callbacks/ model saver is cool too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "class_labels = ['Newborn', '12 Months', '24 Months', '36 Months']\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names=class_labels,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        #plt.colorbar()\n",
    "        #plt.imshow(data, cmap=cmap, vmin=0, vmax=1) \n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # [mwen]\n",
    "\n",
    "    thresh = cm.max() / 2 # [mwen]\n",
    "    thresh_norm = cm_norm.max() / 1.5 # [mwen]\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm_norm[i, j]) + \"\\n{:,}\".format(cm[i, j]), # [mwen]\n",
    "                     horizontalalignment=\"center\", verticalalignment=\"center\", fontsize=12,\n",
    "                     color=\"white\" if cm_norm[i, j] > thresh_norm else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=12, color='darkblue') # fontweight='bold'\n",
    "    plt.xlabel('Predicted label\\nAccuracy={:0.2f}; Misclassification rate={:0.2f}'.format(accuracy, misclass), fontsize=12, color='darkblue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZhlqLDPTYr5",
    "outputId": "f52c05d1-1dd4-4dcc-db76-8984ca2b751e"
   },
   "outputs": [],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global definitions\n",
    "DO_3D = False\n",
    "\n",
    "if DO_3D:\n",
    "    INPUT_SHAPE = (150,150,20,3) \n",
    "else:\n",
    "    INPUT_SHAPE = (150,150,3)\n",
    "\n",
    "NUM_CLASSES = -1 # will be determined from the number of folders later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G71ASkwETYr6"
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = \"F:/Python/Dataset/NIMH/Fusion/dev\"  # Directory of dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These are needed to decide based on the file path if an image belongs to train or valid\n",
    "TRAIN_DIR_NAME = 'train_part'\n",
    "VALID_DIR_NAME = 'valid_part'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe to hold the training/validation data information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_files = []\n",
    "for current_file in Path(DATA_DIR).rglob('*.gz'):\n",
    "    source_files.append(str(current_file).split('\\\\')[-3:])\n",
    "source_files_df = pd.DataFrame(source_files)\n",
    "\n",
    "print(source_files_df.head()) # Add this line to receive the output. It has to have three columns, or the format has to be fixed.\n",
    "\n",
    "#source_files_df.columns = ['a', 'b', 'c', 'd', 'set', 'class', 'filename']\n",
    "source_files_df.columns = ['set', 'class', 'filename']\n",
    "filename_parts = source_files_df['filename'].str.split('_', expand=True) # Note that this is NOT the str.split() method, but a Pandas version! expand=True yields dataframe columns rather than a list.\n",
    "source_files_df['patient_id'] = filename_parts[1]\n",
    "source_files_df['visit'] = filename_parts[2]\n",
    "ending = filename_parts[3].str.split('.', expand=True)\n",
    "source_files_df['contrast'] = ending[0]\n",
    "source_files_df.head()\n",
    "\n",
    "CATEGORIES = list(source_files_df['class'].unique())\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "\n",
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: 'Month0',\n",
    " 1: 'Month12',\n",
    " 2: 'Month24',\n",
    " 3: 'Month36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5 # How many folds to split into\n",
    "\n",
    "patient_ids = source_files_df.patient_id.unique()\n",
    "\n",
    "# For stratified folds, we need to figure out the class for each patient. We assign it to the first, if imaged multiple times.\n",
    "patient_classes = []\n",
    "for patID in patient_ids:\n",
    "    patient_classes.append(source_files_df.loc[source_files_df['patient_id']==patID]['class'].unique()[0])\n",
    "#print(list(zip(patient_ids, patient_classes)))\n",
    "\n",
    "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=4711)\n",
    "\n",
    "# Save the folds yielded by the generator method into an array, in order not to need to wrap the entire training into the folds.\n",
    "folds = []\n",
    "for train_indices, val_indices in kf.split(patient_ids, patient_classes):\n",
    "    folds.append([patient_ids[train_indices], patient_ids[val_indices]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 2D  and 3D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "history = []\n",
    "\n",
    "for TRAIN_ON_FOLD in range(FOLDS):\n",
    "\n",
    "    if DO_3D:\n",
    "        RESCALE_SHAPE = (250,250,30) # From this size, we will discard 50,50,5 from each border to yield the 150x150x20 size.    \n",
    "    else:\n",
    "        RESCALE_SHAPE = (250,250,50) # Don't throw away so much information for 2D\n",
    "    INTERPOL_ORDER = 2\n",
    "\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    valid_data = []\n",
    "    valid_labels = []\n",
    "\n",
    "    # loop over patients.\n",
    "    for patient in tqdm_notebook(patient_ids[:], desc='Patient'): # First, go by patients.\n",
    "        images = source_files_df.loc[source_files_df['patient_id']==patient]\n",
    "        # print(patient, '\\n', images)\n",
    "        classes = images['class'].unique()\n",
    "\n",
    "        # loop over classes this patient is in.\n",
    "        for the_class in classes: # Then by class (age group)\n",
    "            class_images = images.loc[images['class']==the_class]\n",
    "            #print(the_class, '\\n', class_images)\n",
    "            visits = class_images['visit'].unique()\n",
    "\n",
    "            # Make sure there aren't multiple visits.\n",
    "            for visit in visits: # Then by visits...\n",
    "                visit_images = class_images.loc[class_images['visit']==visit]\n",
    "                #print(visit, '\\n', class_images)\n",
    "\n",
    "                # For the fusion data, we expect 3 contrasts (which will end up in the channel dimension)\n",
    "                if len(visit_images)!=3:\n",
    "                    print('should always have three contrasts, but have %d for patient %s, class %s' %(len(visit_images),patient,the_class))\n",
    "                    continue;\n",
    "\n",
    "                if DO_3D: \n",
    "                    image_out_size = (3,150,150,20)\n",
    "                else:\n",
    "                    image_out_size = (3,150,150) # We will discard the border to save space.\n",
    "\n",
    "                multichannel_image = np.zeros(image_out_size)\n",
    "                multichannel_image_flipped = np.zeros(image_out_size)\n",
    "                multichannel_image_rotL = np.zeros(image_out_size)\n",
    "                multichannel_image_rotR = np.zeros(image_out_size)\n",
    "                multichannel_image_rand_rotL = np.zeros(image_out_size)\n",
    "                multichannel_image_rand_rotR = np.zeros(image_out_size)\n",
    "\n",
    "                # now read, preprocess, cut, average, and concatenate the images of one visit\n",
    "                for idx, (index,image) in enumerate(visit_images.iterrows()):\n",
    "                    #print(os.path.join(DATA_DIR,image['set'],image['class'],image['filename']))\n",
    "                    nii_data = nb.load(os.path.join(DATA_DIR,image['set'],image['class'],image['filename'])) # compose the filename from the current dataframe entry\n",
    "                    np_data = np.asarray(nii_data.dataobj)\n",
    "\n",
    "                    # resize data using ndimage from scipy (https://docs.scipy.org/doc/scipy/reference/ndimage.html)\n",
    "                    original_shape = np_data.shape\n",
    "                    scale = [(RESCALE_SHAPE[i] + 0.0)/original_shape[i] for i in range(len(original_shape))]\n",
    "                    out_data = ndimage.interpolation.zoom(np_data, scale, mode= \"nearest\", order = INTERPOL_ORDER)\n",
    "                    out_data_mean = np.mean(out_data[50:200,50:200,int(out_data.shape[2]/2-2):int(out_data.shape[2]/2+2)], axis=2) # Select some slices in the middle and average.\n",
    "                    rand_rotL = np.random.randint(10,90)\n",
    "                    rand_rotR = np.random.randint(10,90)\n",
    "                    if DO_3D:\n",
    "                        multichannel_image[idx] = cv2.normalize(out_data[50:200,50:200,5:25], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                        if patient in folds[TRAIN_ON_FOLD][0]: # Only augment training data\n",
    "                            multichannel_image_flipped[idx] = cv2.normalize(out_data[50:200,50:200,5:25], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rotL[idx] = cv2.normalize(scipy.ndimage.rotate(out_data[50:200,50:200,5:25],-10, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rotR[idx] = cv2.normalize(scipy.ndimage.rotate(out_data[50:200,50:200,5:25], 10, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rand_rotL[idx] = cv2.normalize(scipy.ndimage.rotate(out_data[50:200,50:200,5:25],-rand_rotL, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rand_rotR[idx] = cv2.normalize(scipy.ndimage.rotate(out_data[50:200,50:200,5:25], rand_rotR, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                    else:\n",
    "                        multichannel_image[idx] = cv2.normalize(out_data_mean, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                        if patient in folds[TRAIN_ON_FOLD][0]: # Only augment training data\n",
    "                            multichannel_image_flipped[idx] = cv2.normalize(out_data_mean[::-1], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rotL[idx] = cv2.normalize(scipy.ndimage.rotate(out_data_mean, -10, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rotR[idx] = cv2.normalize(scipy.ndimage.rotate(out_data_mean, 10, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rand_rotL[idx] = cv2.normalize(scipy.ndimage.rotate(out_data_mean, -rand_rotL, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                            multichannel_image_rand_rotR[idx] = cv2.normalize(scipy.ndimage.rotate(out_data_mean, rand_rotR, reshape=False), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "                # Assign to either train or valid data\n",
    "                if patient in folds[TRAIN_ON_FOLD][0]:\n",
    "                    train_data.append([np.moveaxis(multichannel_image,0,3 if DO_3D else 2), CATEGORIES.index(the_class)])  # add this to our train_data            \n",
    "                    train_data.append([np.moveaxis(multichannel_image_flipped,0,3 if DO_3D else 2), CATEGORIES.index(the_class)]) \n",
    "                    train_data.append([np.moveaxis(multichannel_image_rotL,0,3 if DO_3D else 2), CATEGORIES.index(the_class)])\n",
    "                    train_data.append([np.moveaxis(multichannel_image_rotR,0,3 if DO_3D else 2), CATEGORIES.index(the_class)])\n",
    "                    train_data.append([np.moveaxis(multichannel_image_rand_rotL,0,3 if DO_3D else 2), CATEGORIES.index(the_class)])\n",
    "                    train_data.append([np.moveaxis(multichannel_image_rand_rotR,0,3 if DO_3D else 2), CATEGORIES.index(the_class)])\n",
    "                else:\n",
    "                    valid_data.append([np.moveaxis(multichannel_image,0,3 if DO_3D else 2), CATEGORIES.index(the_class)])\n",
    "                    \n",
    "                    \n",
    "    \n",
    "    # Create traain and valid\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for features,label in train_data:\n",
    "        X_train.append(features)\n",
    "        y_train.append(label)\n",
    "\n",
    "    if DO_3D:\n",
    "        X_train = np.array(X_train).reshape(-1, INPUT_SHAPE_3D[0], INPUT_SHAPE_3D[1], INPUT_SHAPE_3D[2], INPUT_SHAPE_3D[3])\n",
    "    else:\n",
    "        X_train = np.array(X_train).reshape(-1, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2])\n",
    "    y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "    print (\"x_train shape: \", X_train.shape)\n",
    "    print (\"y_train shape: \", y_train.shape)\n",
    "\n",
    "    X_valid = []\n",
    "    y_valid = []\n",
    "\n",
    "    for features,label in valid_data:\n",
    "        X_valid.append(features)\n",
    "        y_valid.append(label)\n",
    "\n",
    "    if DO_3D:\n",
    "        X_valid = np.array(X_valid).reshape(-1, INPUT_SHAPE_3D[0], INPUT_SHAPE_3D[1], INPUT_SHAPE_3D[2], INPUT_SHAPE_3D[3])\n",
    "    else:\n",
    "        X_valid = np.array(X_valid).reshape(-1, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2])\n",
    "    y_valid = to_categorical(y_valid, num_classes=NUM_CLASSES)\n",
    "    print (\"x_test shape: \", X_valid.shape)\n",
    "    print (\"y_test shape: \", y_valid.shape)\n",
    "\n",
    "    \n",
    "    np.save('xtrain-aug_%d.npy'%TRAIN_ON_FOLD, X_train)\n",
    "    np.save('xvalid_%d.npy'%TRAIN_ON_FOLD, X_valid)\n",
    "    np.save('ytrain-aug_%d.npy'%TRAIN_ON_FOLD, y_train)\n",
    "    np.save('yvalid_%d.npy'%TRAIN_ON_FOLD, y_valid)    \n",
    "\n",
    "####################################################################################################################    \n",
    "    #CNN Model\n",
    "    from keras.constraints import max_norm\n",
    "    from tensorflow.keras.layers import Dense, Activation, Dropout,SpatialDropout2D, SpatialDropout3D\n",
    "\n",
    "    num_classes=4\n",
    "\n",
    "\n",
    "\n",
    "    def Conv(filters=16, kernel_size=(3,3), activation='relu', input_shape=None):\n",
    "        if input_shape:\n",
    "            return Conv2D(filters=filters, kernel_size=kernel_size, \n",
    "                          activation=activation, input_shape=input_shape) \n",
    "        else:\n",
    "            return Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                          activation=activation)\n",
    "\n",
    "    def Conv_3d(filters=16, kernel_size=(3,3,3), activation='relu', input_shape=None):\n",
    "        if input_shape:\n",
    "            return Conv3D(filters=filters, kernel_size=kernel_size, \n",
    "                          activation=activation, input_shape=input_shape)\n",
    "        else:\n",
    "            return Conv3D(filters=filters, kernel_size=kernel_size,\n",
    "                          activation=activation)\n",
    "\n",
    "\n",
    "    # Define 2D Model\n",
    "    def CNN(input_dim, num_classes): \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv(64, (3,3), input_shape=(150,150,3)))\n",
    "        model.add(Conv(64, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))    \n",
    " \n",
    "\n",
    "        model.add(Conv(64, (3,3)))\n",
    "        model.add(Conv(64, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "\n",
    "        model.add(Conv(128, (3,3)))\n",
    "        model.add(Conv(128, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        #model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(128, activation='relu',  kernel_constraint=max_norm(2.)))\n",
    "        model.add(Dropout(rate=0.7))\n",
    "        model.add(Dense(128, activation='relu',  kernel_constraint=max_norm(2.)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        return model\n",
    "\n",
    "###################################################################################################       \n",
    "    # Define 3D Model\n",
    "    def CNN_3d(input_shape = (150,150,20,3), num_classes=4):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv_3d(32, input_shape=(150,150,20,3)))\n",
    "        model.add(Conv_3d(32))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool3D(pool_size=(2,2,2)))\n",
    "        #model.add(MaxPool3D(pool_size=(2)))\n",
    "        model.add(MaxPool3D())\n",
    "        model.add(Dropout(0.2))    \n",
    "\n",
    "        model.add(Conv_3d(64))\n",
    "        model.add(Conv_3d(64))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool3D(pool_size=(2,2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv_3d(96))\n",
    "        model.add(Conv_3d(96))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool3D(pool_size=(2,2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv_3d(128))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(MaxPool3D())\n",
    "        #model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "        model.add(GlobalAveragePooling3D())\n",
    "        #model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        return model\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "    # 2D variants\n",
    "    model = CNN(INPUT_SHAPE, NUM_CLASSES)\n",
    "\n",
    "    # 3D variants\n",
    "    #model = CNN_3d(INPUT_SHAPE_3D, NUM_CLASSES)\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "    non_trainable_count = np.sum([K.count_params(w) for w in model.non_trainable_weights])\n",
    "    print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "    print('Trainable params: {:,}'.format(trainable_count))\n",
    "    print('Non-trainable params: {:,}'.format(non_trainable_count))    \n",
    "    \n",
    "####################################################################################################\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "    # Make loss more expensive for the tougher classes.\n",
    "\n",
    "    earlystop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    model.compile(loss=categorical_crossentropy,  optimizer= optimizer , metrics=['acc'])\n",
    "\n",
    "    history.append(model.fit(X_train, y_train, batch_size=8, epochs=300, validation_data=(X_valid,y_valid), class_weight=class_weight, callbacks=[earlystop_cb]))\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "    #Confusion Matrix and Classification Report\n",
    "\n",
    "    pred.append(model.predict(X_valid, batch_size=8)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apart from now postprocessing the predictions for the desired fold, nothing has to change..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion Matrix and Classification Report for individual folds\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "for WHICH_FOLD in range(FOLDS):\n",
    "\n",
    "    y_pred = np.argmax(pred[WHICH_FOLD], axis=1)\n",
    "    y_pred_all.append(y_pred)\n",
    "    # Load corresponding y_valid data\n",
    "    y_true = np.load(str('yvalid_%d.npy'%WHICH_FOLD))\n",
    "\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_true_all.append(y_true)\n",
    "\n",
    "    class_labels = ['Newborn', '12 Months','24 Months', '36 Months']\n",
    "    print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics from all validation runs    \n",
    "flat_y_pred_all = [item for sublist in y_pred_all for item in sublist]\n",
    "flat_y_true_all = [item for sublist in y_true_all for item in sublist]\n",
    "cm = confusion_matrix(flat_y_true_all, flat_y_pred_all)\n",
    "class_labels = ['Newborn', '12 Months','24 Months', '36 Months']\n",
    "print(\"Summed Metrics:\")\n",
    "print(classification_report(flat_y_true_all, flat_y_pred_all, target_names=class_labels))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOnfTdceTYsC"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycm import *\n",
    "\n",
    "cm = ConfusionMatrix(flat_y_true_all, flat_y_pred_all)\n",
    "cm.table\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracy and los for 5fold \n",
    "losses = []\n",
    "val_losses   = []\n",
    "accs = []\n",
    "val_accs   = []\n",
    "for hist in history:\n",
    "    losses.append(hist.history['loss'])\n",
    "    val_losses.append(hist.history['val_loss'])\n",
    "    accs.append(hist.history['acc'])\n",
    "    val_accs.append(hist.history['val_acc'])\n",
    "\n",
    "shortest = min([len(l) for l in losses])\n",
    "print ('Shortest training:', shortest)\n",
    "\n",
    "nlosses = [l[:shortest] for l in losses]\n",
    "nval_losses = [l[:shortest] for l in val_losses]\n",
    "naccs = [l[:shortest] for l in accs]\n",
    "nval_accs = [l[:shortest] for l in val_accs]\n",
    "\n",
    "mean_l = np.mean(nlosses, axis=0)\n",
    "mean_vl = np.mean(nval_losses, axis=0)\n",
    "mean_a = np.mean(naccs, axis=0)\n",
    "mean_va = np.mean(nval_accs, axis=0)\n",
    "min_l = np.min(nlosses, axis=0)\n",
    "min_vl = np.min(nval_losses, axis=0)\n",
    "min_a = np.min(naccs, axis=0)\n",
    "min_va = np.min(nval_accs, axis=0)\n",
    "max_l = np.max(nlosses, axis=0)\n",
    "max_vl = np.max(nval_losses, axis=0)\n",
    "max_a = np.max(naccs, axis=0)\n",
    "max_va = np.max(nval_accs, axis=0)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_l, '-r', label=\"Train loss\")\n",
    "plt.fill_between(range(shortest), min_l, max_l,\n",
    "                 color='lightcoral', alpha=0.4)\n",
    "plt.plot(mean_vl, '-b', label=\"Val loss\")\n",
    "plt.fill_between(range(shortest), min_vl, max_vl,\n",
    "                 color='cornflowerblue', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"Train/Valid Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_a, '-r', label=\"Train acc\")\n",
    "plt.fill_between(range(shortest), min_a, max_a,\n",
    "                 color='lightcoral', alpha=0.4)\n",
    "plt.plot(mean_va, '-b', label=\"Val acc\")\n",
    "plt.fill_between(range(shortest), min_va, max_va,\n",
    "                 color='cornflowerblue', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"Train/Valid Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to plot one specific fold\n",
    "# plot Loss\n",
    "WHICH_FOLD=1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history[WHICH_FOLD].history['loss'], label=\"Train loss\")\n",
    "plt.plot(history[WHICH_FOLD].history['val_loss'], label=\"Val loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Train/Valid Loss (fold %d)\"%WHICH_FOLD)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy for one specific fold\n",
    "WHICH_FOLD=1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history[WHICH_FOLD].history['acc'], label=\"Train acc\")\n",
    "plt.plot(history[WHICH_FOLD].history['val_acc'], label=\"Val acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Train/Valid Acc (fold %d)\"%WHICH_FOLD)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7ABMm86TYsD"
   },
   "outputs": [],
   "source": [
    "# Statistical results\n",
    "from pycm import *\n",
    "\n",
    "cm = ConfusionMatrix(flat_y_true_all, flat_y_pred_all)\n",
    "cm.table\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2D CNN Keras T1&T2 &PD -3 Feb- Acc 30 - with separate valid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
